# 2.1.3. Các hàm kích hoạt trong CNN

## 1. ReLU (Rectified Linear Unit)

### Định nghĩa và công thức

-   f(x) = max(0, x)
-   Là hàm kích hoạt phi tuyến phổ biến nhất trong CNN

### Ưu điểm

-   Giải quyết vấn đề vanishing gradient
-   Tính toán đơn giản và nhanh chóng
-   Tạo tính phi tuyến cho mạng

### Các biến thể

-   Leaky ReLU: f(x) = max(0.01x, x)
-   ELU (Exponential Linear Unit)
-   PReLU (Parametric ReLU)

## 2. Softmax

### Định nghĩa và công thức

-   Chuyển đổi vector thành phân phối xác suất
-   Sử dụng trong lớp đầu ra cho bài toán phân loại

### Ứng dụng

-   Phân loại đa lớp
-   Dự đoán xác suất cho mỗi lớp
-   Kết hợp với hàm mất mát cross-entropy

### So sánh với các hàm kích hoạt khác

-   Sigmoid
-   Tanh
-   Ưu điểm và nhược điểm

## 3. Các hàm kích hoạt khác

### Sigmoid

-   Công thức và đặc điểm
-   Ứng dụng và hạn chế

### Tanh

-   Công thức và đặc điểm
-   So sánh với Sigmoid

### Các hàm kích hoạt mới

-   Swish
-   Mish
-   GELU

## Tài liệu tham khảo

1. Nair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. ICML.
2. He, K., et al. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. ICCV.
3. Ramachandran, P., et al. (2017). Searching for activation functions. arXiv preprint arXiv:1710.05941.
4. Misra, D. (2019). Mish: A self regularized non-monotonic activation function. arXiv preprint arXiv:1908.08681.
