# 2.1.3. Các hàm kích hoạt trong CNN

## Giới thiệu

Các hàm kích hoạt (Activation Functions) đóng vai trò quan trọng trong việc đưa tính phi tuyến vào mạng nơ-ron, cho phép mạng học các mối quan hệ phức tạp trong dữ liệu. Theo Goodfellow et al. (2016), việc lựa chọn hàm kích hoạt phù hợp là một yếu tố quan trọng quyết định hiệu suất và khả năng học của mạng nơ-ron.

## ReLU (Rectified Linear Unit)

### Định nghĩa và đặc điểm

ReLU là một trong những hàm kích hoạt phổ biến nhất trong CNN. Theo LeCun et al. (2015), ReLU được định nghĩa là f(x) = max(0, x), có khả năng giải quyết vấn đề vanishing gradient và đơn giản hóa quá trình huấn luyện mạng.

### Ưu điểm

1. Tính toán đơn giản và nhanh chóng
2. Giải quyết vấn đề vanishing gradient
3. Tạo ra tính phi tuyến cần thiết
4. Giảm thiểu overfitting

### Nhược điểm

1. Dying ReLU problem: Các nơ-ron có thể bị "chết" khi gradient bằng 0
2. Không có tính đối xứng quanh 0
3. Không có giới hạn trên cho giá trị đầu ra

## Các biến thể của ReLU

### Leaky ReLU

Leaky ReLU được giới thiệu để khắc phục vấn đề dying ReLU. Theo He et al. (2016), hàm này cho phép một gradient nhỏ cho các giá trị âm, giúp các nơ-ron tiếp tục học ngay cả khi đầu vào âm.

### ELU (Exponential Linear Unit)

ELU là một biến thể cải tiến của ReLU, được đề xuất bởi Clevert et al. (2016). Hàm này có khả năng tạo ra gradient âm, giúp đẩy giá trị trung bình của kích hoạt gần với 0 hơn, cải thiện tốc độ học.

### PReLU (Parametric ReLU)

PReLU là phiên bản học được của Leaky ReLU, cho phép mạng tự động học hệ số độ dốc cho phần âm. Theo He et al. (2016), PReLU đã chứng minh hiệu quả vượt trội trong các ứng dụng thực tế.

## Softmax

### Định nghĩa và ứng dụng

Softmax là hàm kích hoạt đặc biệt được sử dụng trong lớp đầu ra của CNN cho bài toán phân loại đa lớp. Theo Krizhevsky et al. (2012), softmax chuyển đổi các giá trị đầu ra thành xác suất phân phối, tổng bằng 1.

### Đặc điểm

1. Chuyển đổi đầu ra thành xác suất
2. Phù hợp cho bài toán phân loại đa lớp
3. Dễ dàng tính toán gradient
4. Tương thích tốt với hàm mất mát cross-entropy

## So sánh các hàm kích hoạt

### Hiệu suất trong thực tế

Theo Simonyan và Zisserman (2015), ReLU và các biến thể của nó thường cho kết quả tốt hơn các hàm kích hoạt truyền thống như sigmoid và tanh trong các ứng dụng CNN. Tuy nhiên, việc lựa chọn hàm kích hoạt phù hợp phụ thuộc vào đặc điểm cụ thể của bài toán.

### Các yếu tố ảnh hưởng

1. Tốc độ hội tụ
2. Độ chính xác của mô hình
3. Khả năng khái quát hóa
4. Tính ổn định trong quá trình huấn luyện

## Kết luận

Việc lựa chọn hàm kích hoạt phù hợp là một yếu tố quan trọng trong thiết kế CNN. Theo Raghu và Schmidt (2020), sự phát triển của các hàm kích hoạt mới tiếp tục cải thiện hiệu suất và khả năng học của mạng nơ-ron, mở ra nhiều khả năng ứng dụng mới trong thị giác máy tính.
